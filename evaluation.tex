% Meyer: Ergebnisse reflektieren, Diskussion (Titel: Test und Diskussion der Ergebnisse oder zwei Hauptkapitel)

Nach Abschluss der Realisierung aller zuvor erstellten Planungen, folgt nun im letzten großen Schritt die Auswertung der erzielten Ergebnisse. Ein entscheidendes Ziel dieser Arbeit ist das Finden einer möglichst guten Optimierung zu der zu Beginn gesetzten Problemstellung. Zweck dieses Abschnittes ist es nun also, zu zeigen ob bzw. wie gut dieses Ziel erreicht wurde. Dazu werden zunächst quantitative Auswertungen anhand wichtiger und auch im folgenden bestimmter Metriken angestellt. Anschließend folgt dann eine qualitative Diskussion mit Bezug auf das gesteckte Ziel.

Um zunächst einmal einen Eindruck der Effektivität und der Ergebnisse der einzelnen Algorithmen zu erhalten, sollen diese zunächst getrennt voneinander zahlenmäßig ausgewertet werden. Dies ist auch sinnvoll, da das Vorgehen und die Ergebnisse durchaus unterschiedlich sind und sich nicht sofort und direkt miteinander vergleichen lassen.

\subsection{Auswertung Algorithmus 1 (RS)}

\input{evaluation1}


\subsection{Auswertung Algorithmus 2 (TSP)}

\input{evaluation2}


\subsection{Vergleich der Lösungen}




\subsubsection{Fazit Algorithmus 1}

Nach der Durchführung der Testszenarien zum Vergleich der verschiedenen Planungsverfahren haben sich tatsächlich erstaunliche Ergebnisse ergeben. Ausgewählt wurden die betrachteten Verfahren bei der Planung, da sie bestimmten Heuristiken folgen, welche in der Theorie als sinnvoll und vor allem besser geeignet als das bisherige, unoptimierte Vorgehen erschienen. Es hat sich aber gezeigt, dass sich nicht alle Verfahren in der Praxis so verhalten, wie es in der Planung vermutet wurde. Dies war aber auch genau das Ziel dieser Auswertung.

Alle Verfahren teilen sich das Problem, dass sie relativ große Schwankungen und Streuungen in der Qualität der Ergebnisse aufweisen. Da dies bei allen Verfahren teils mehr, teils etwas moderater auftritt, lässt sich vermuten, dass dies zum großen Teil auch auf Einsatzsituation zurückzuführen ist. Es lässt sich leider nicht vermeiden, dass die Eingangsdaten einer relativ großen Willkür und Zufälligkeit unterliegen. Sicherlich ließe sich diese etwas minimieren, indem noch genauere Analysen, beispielsweise zur Verteilung und relativen Anzahl der Abfertigungskategorien zueinander angestellt würden. Eine allgemeingültige Lösung ist deutlich schwieriger zu optimieren, dies ist aber im betrachteten Kontext auch nicht unbedingt anders möglich. Das reale Problemfeld ist nun einmal sehr unübersichtlich und komplex, mit Einschränkungen und Annahmen lässt sich dies nur bedingt sinnvoll ändern.

Insbesondere das SJN Verfahren hat sich nicht besonders tauglich als allgemeine Lösung gezeigt. Es ist einfach sehr stark davon abhängig, welche Aufgaben wie lange brauchen. Für einige Fälle mag dieses Verfahren gut funktionieren, aber in vielen Fällen wird es Abfertigungskategorien geben, welche immer schneller gehen als andere, wodurch es einseitige Verteilungen gibt. Die schnellere Abfertigung und geringere Wartezeit ist sicherlich ein Vorteil dieses Ansatzes, dies war aber nicht Hauptaugenmerk der Optimierung und bringt schlussendlich auch gar keine allzu großen Vorteile. Die Idee einer festen Planung war es nämlich auch, den LKW Fahrern feste Zeitpunkte nennen zu können, in denen sie ankommen sollen. Ob diese nun zu Beginn nah beieinander liegen oder sehr verteilt ist dann irrelevant. Die beiden anderen Verfahren MIT und FCFS sind sich dagegen wesentlich ähnlicher als angenommen. Die Vorteil, welcher in der Planung im MIT Verfahren gesehen wurde war, dass die Auslastung deutlich besser wird, wenn zunächst Aufgaben verplant werden, welche wenig genutzte Ressourcen benötigen. Das FCFS Verfahren war zusätzlich eher als interessanter Vergleich gedacht, wie sich eine ganz zufällige Verteilung auswirkt. Aus der Ähnlichkeit der Ergebnisse lässt sich schlussfolgern, dass dieser Gedanke gar nicht so entscheidend für das Ergebnis ist. Vielmehr scheint es, gerade im Vergleich zum SJN Verfahren zu bringen, die Aufgaben aus unterschiedlichen Kategorien gleichmäßig zu verplanen. Dies scheint die Leerlaufzeiten schon sehr gut auszufüllen. Insbesondere das FCFS Verfahren zeigt bei der Verbesserung der Anzahl von abgefertigten LKW (siehe Abb. \ref{fig:eal3}) eine größere Steigerung als beim MIT Ansatz. Auch hier wird eine relativ gleichmäßige Verteilung der Kategorien durch das Planen der weniger genutzten Ressourcen erreicht. Interessanterweise ist allerdings eine komplett zufällige Planung in diesem Fall immer noch ein kleines bisschen besser. 

Den allergrößten Optimierungsvorteil scheint man allerdings schon ganz einfach dadurch erreichen zu können, dass man den LKW Fahrern feste Zeitpunkte zuweist und eben keine zufällige Ankunft innerhalb des Slots ermöglicht. So lässt sich viel konkreter und mit viel weniger Unsicherheit eine Planung erstellen. 
Insbesondere die Leerlaufzeiten zwischen den Aufträgen lassen sich dadurch bei allen Verfahren stark minimieren (Abb. \ref{fig:evalLeerlaufzeit}) und eine gezieltere Auslastung der Ressourcen kann erreicht werden.

Will man aus dieser Evaluation nun auf ein Verfahren festlegen, welches weiter genutzt werden sollte, so wäre dies der FCFS Ansatz. In Kombination mit einer moderaten Überbuchung lassen sich hier durchaus gut verbesserte Planungen erzeugen, was die Anzahl der bearbeiteten LKW und auch die Leerlaufzeiten angeht. Eine möglichst gute Verteilung der Ressourcen ist im Vorhinein definitiv gut und trägt zu noch besseren Ergebnissen bei, eine Verbesserung lässt sich aber auch bei moderaten Ungleichverteilungen erzielen.

\subsubsection{Fazit Algorithmus 2}

Zunächst einmal kann man festhalten, dass es in dieser Evaluation im Gegensatz zum anderen Algorithmus in erster Linie nicht um die Bewertung verschiedener Verfahren innerhalb der Implementierung ging. Dort wurde der Effekt verschiedener heuristischer Ansätze untersucht. Hier ist es dagegen so, dass das zugrunde liegende Traveling Salesman Problem zumindest theoretisch immer perfekt gelöst werden kann. Zu Beginn ging es in den drei Testszenarien also eher darum, eine Einschätzung zu geben, wie geeignet die drei beispielhaft implementierten Lösungsverfahren für das TSP sind. Dabei hat sich schnell herausgestellt, dass die beiden exakten Branch and Bound bzw. Reduced Matrix Verfahren zumindest in der hier implementierten Form nicht geeignet sind, um die für die Problemstellung benötigten Graphen zu lösen. Für das Simulated Annealing Verfahren konnte gezeigt werden, dass es spätestens nach mehreren Durchläufen zumindest nahezu das perfekte Ergebnis liefert. Somit war das Finden eines geeigneten Lösungsverfahrens zwar ein wichtiges Ziel dieser Evaluation, unter der Annahme, dass alle Verfahren das gleiche und beste Ergebnis liefern, sind die anschließenden Testszenarien allerdings unabhängig vom eigentlichen Lösungsverfahren gültig und zeigen allgemein wie gut der Ansatz ist, das in der Arbeit betrachtete Problem mit Hilfe des Traveling Salesman Problems lösen.

%Lösungsverfahren
Bezüglich der Lösungsverfahren lässt sich festhalten, dass grundsätzlich alle geeignet erscheinen, das TSP und somit auch die vorliegende Problemstellung zu lösen. Insbesondere die beiden exakten Verfahren zeigen aber einen sehr großen Zeit bzw. Rechenressourcen-Bedarf mit steigender Zahl von Knoten im zu lösenden Graphen. Dies war auch schon nach der theoretischen Betrachtung zu erwarten, allerdings wurden die Grenzen noch schneller erreicht, als vermutet. Theoretisch sollten die Verfahren für Graphen mit etwa xyz Knoten geeignet sein\todo{Zahl und Quelle heraussuchen}. Wie die Versuche gezeigt haben hängt diese Größe auch sehr von den zur Verfügung stehenden Rechenressourcen ab. Ein weiterer wichtiger Aspekt ist aber wahrscheinlich die Implementierung der Verfahren. Durch die Umsetzung einer wesentlich effizienteren und Zeit- bzw. Ressourcensparenderen Lösung, möglicherweise sogar in einer für solche Rechenoperationen besser geeigneten Programmiersprache als Java, ist hier sicherlich noch viel Potenzial herauszuholen. In der Gegebenen Zeit dieser Arbeit war es nicht möglich und auch nicht das Ziel, hier ein perfektes und hoch optimiertes Verfahren zu implementieren. Viel mehr sollte gezeigt werden, dass die Planung und der allgemeine Ansatz zur Lösung des vorliegenden Problems geeignet und umsetzbar sind. Dazu haben die hier genutzten, rudimentären Implementierungen vollkommen ausgereicht. Sollte man diesen Ansatz weiter verfolgen wollen und ihn in einer realen Implementierung einsetzen, dann kann der modulare Aufbau des Java-Projekts genutzt werden und eine entsprechend verbesserte oder sogar ganz andere Implementierung eingesetzt werden. Für die weitere Analyse bietet der Simulated Annealing Ansatz allerdings eine gute Basis. Unter Verwendung der optimierten Parameter und durch wiederholtes Durchlaufen des Algorithmus lässt sich eine nahezu perfekte Lösung bei sehr geringem Rechenzeitbedarf erzielen. Selbst wenn sich ein noch besseres Verfahren implementieren ließe, würde dies die anschließenden Auswertungen nur noch etwas weiter verbessern.

%Bewertung effektivität des algorithmus
Zu dem Algorithmus selbst lassen sich aus den Testszenarien, aber auch aus den Erfahrungen aus der Planung und Implementierung einige Schlussfolgerungen ziehen. Zunächst einmal konnte gezeigt werden, dass sich auch im praktischen Einsatz eine gute Optimierung erzielen lässt. Die Dauer der Abfertigung einzelner LKW kann verringert werden, wodurch bei weiterer Füllung eines Slots 40-60\% mehr LKW eingeplant und bearbeitet werden können. Eine Überbuchung der Slots scheint hier im Gegensatz zum anderen Algorithmus nur bedingt sinnvoll und muss im realen Einsatz genau abgewogen werden. Insgesamt funktioniert das hier genutzte Verfahren am besten und arbeitet am effektivsten, wenn ein gewisser Planungsspielraum gelassen wird, d.h. wenn eine gewisse Slotgröße und eine gewisse Zahl an Ladeplätzen vorhanden ist. So lässt sich einfach die beste Verteilung der LKW erzielen. Ein Nachteil der Nutzung des Traveling Salesman Problems, insbesondere des mTSP für mehrere Ladeplätze ist es, dass hier zwar die insgesamt die kürzeste Bearbeitungsdauer ermittelt wird, dadurch entsteht aber auf die einzelnen Plätze bezogen nicht zwangsweise eine gleichmäßige Verteilung. Die Lösung, welche hierfür geplant und umgesetzt wurde ist, dass zunächst eine Aufteilung auf unbegrenzte Slots erfolgt und dann eine Limitierung bzw. Umverteilung erfolgt, sodass alle Ladeplätze für die Zeit des Slots so gut wie möglich ausgelastet sind. Dieses Vefahren wurde schon so geplant, dass eine möglichst effektive Reihenfolge bestehen bleibt, das best mögliche Ergebnis entsteht dadurch aber vermutlich nicht. Dies war aber die Einschränkung, welche in Kauf genommen wurde, da hier nun einmal das mTSP als Ansatz genutzt wurde, welches eben die insgesamt besten Kosten und nicht die beste Verteilung zum Ziel hat. Insgesamt sind die Auswertungen eher als Nachweis zu sehen, dass die Idee grundsätzlich funktionieren kann. Es wurden allgemein sehr viele Annahmen getroffen, insbesondere auch was den Zeitbedarf der Abfertigung bestimmter Güter angeht. Mit anderen Werten, welche sich in einem realen Einsatz ergeben werden, könnten hier sicherlich noch etwas andere Zahlen herauskommen. Es lassen sich außerdem teils sehr große Schwankungen und Streuungen in den Ergebnissen erkennen. Wie schon im anderen Algorithmus geschlussfolgert wurden, ist es auch hier so, dass eine relativ große Willkür und Zufälligkeit bei den Eingangsdaten besteht. Dies wurde an die Praxis angelehnt und lässt sich auch da nicht vermeiden, sodass es allgemein sehr schwierig ist, konstant gute oder gleich gut Verbesserte Ergebnisse zu erzielen. Das liegt einfach in der Natur der Problemstellung.

Das Fazit zu diesem Algorithmus ist also, dass die Idee, eine möglichst kurze Abfertigungszeit zu erzielen, indem man schau, welche LKW am schnellsten nacheinander bearbeitet werden können, durchaus funktioniert. Setzt man ein passendes Lösungsverfahren für das TSP ein, so können sichtbar verbesserte Zeitplanungen erzeugt werden.


\subsubsection{Vergleich der Algorithmen}

Ein direkter, vor allem quantitativer Vergleich der beiden Algorithmen ist kaum sinnvoll möglich. Beide sind in ihren konzipierten Einsatzbereichen geeignet und können verbesserte Ergebnisse erzielen. Allerdings sind die Ansatzpunkte und der Fokus beider Algorithmen durchaus verschieden. Auch die verwendeten Zeiten, insbesondere die angenommenen Abfertiungszeiten pro Kategorie basieren auf Annahmen aus der vorherigen Analyse und sind kaum direkt miteinander zu vergleichen. Welches Optimierungskonzept schlussendlich eingesetzt werden sollte, hängt also vermutlich eher von den schlussendlichen, realen Anforderungen ab und wie sich diese in die Praxis integrieren lassen.


\subsection{Gesamtbewertung}

Nachdem nun die zu Beginn gestellte Problemstellung ausführlich analysiert, geplant, implementiert und bewertet wurde, sollen an dieser Stelle noch einmal alle Erkenntnisse zusammengefasst werden. Über den ganzen Entwicklungsprozess konnten mit Blick auf die bearbeitete Problematik sehr viele Aspekte und Erkenntnisse gewonnen werden. Der ganze Prozess diente dazu, eine prototypische Optimierung des gegebenen Problems zu Entwickeln und auf dem Wege Erkenntnisse bezüglich einer möglichen Lösung zu erlangen und zu erarbeiten. 

Schon bei der Analyse ist dabei aufgefallen, dass die Ausgangssituation und damit auch eine potenzielle Lösung durchaus komplex und nicht trivial und geradlinig umzusetzen ist. Es mussten schon zu Beginn der Planungsphase Ideen entwickelt werden, wie all diese Aspekte in ein Verfahren bzw. einen Algorithmus überführt werden können, welcher dann als Software implementiert werden kann. Bei der Recherche zeigte sich schnell, dass es der einzig wirklich erfolgversprechende Weg ist, bestehende Verfahren mit bereits zumindest teilweise vorhandenen Lösungsansätzen einzusetzen. Es sind einfach so viele Parameter und Einschränkungen zu bedenken, dass anders keine sinnvolle und vor allem in dem gegebenen Zeitrahmen erfolgversprechende Lösung zu erreichen war. Es hat sich allerdings ebenfalls schnell herausgestellt, dass auch die Nutzung von bereits bekannten Verfahren diverse Probleme mit sich bringt. Eine direkte Adaption auf das hier gegebene Problem mit all seinen Facetten ist auch hier nicht möglich. Es war also nötig, die Problemstellung durch passende Einschränkungen so zu formen, dass fertige Verfahren zur Lösung angewendet werden können. Um hier einen Spielraum zu haben und unter den Bedingungen eine geeignete Lösung finden zu können wurden die zwei Algorithmen parallel implementiert. Beispielsweise die Nutzung des Traveling Salesman Problems wurde schnell als scheinbar gut passende Lösung gefunden. Das Finden einer optimalen Reihenfolge für möglichst geringe Kosten und bereits gut bekannte Lösungsverfahren erscheinen zunächst als sehr guter Ausgangspunkt, allerdings zeigte sich auch hier bei genauerer Betrachtung, dass Einschränkungen wie begrenzte Slots nur unter Umwegen adaptierbar sind. Allerdings mussten einige Anforderungen, wie möglicherweise verschiedene nutzbare Geräte für eine Abfertigungskategorie, hier weg abstrahiert werden, um überhaupt die Chance auf eine funktionierende Lösung zu haben. Auch bei dem anderen Verfahren, welches versucht, einen Zeitplan über Heuristiken zu erstellen, haben sich solche Probleme aufgetan. Hier konnte der Aspekt mit Wechselzeiten und entsprechenden Optimierungen durch passende Reihenfolge nicht berücksichtigt werden, ansonsten wäre es auch hier sehr schwer gewesen, sinnvolle Ergebnisse unter Berücksichtigung aller Anforderungen zu erzielen.

Der hohen Zahl von Einschränkungen und Anforderungen an ein mögliches Ergebnis stehen immer die eher Willkürlichen und wenig planbaren Eingabedaten gegenüber. Dadurch, dass die LKW in Ankunftszeit und Warenart aus Terminal sich sehr zufällig und wenig planbar kommen, ist auch eine gute Planung sehr schwer zu realisieren. Dadurch zeigt sich auch in vielen der vorangegangenen Auswertungen eine hohe Streuung und Variabilität in den Ergebnissen. Es gibt Fälle, in denen sehr gute Verbesserungen erzielt werden können, genau so gut kommt es aber auch oft vor, dass die prozentuale Verbesserung sehr gering ausfällt. In der Ausgangssituation kommen LKW innerhalb ihres gebuchten Slots an, wann sie wollen. Diese Praxis beizubehalten ist nicht möglich, wenn in irgendeiner Form eine sinnvolle Vorausplanung erzeugt werden soll. Es hat sich über die ganze Entwicklung gezeigt, dass eine optimale Abfertigung nur möglich ist, wenn ein konkreter Zeitplan erstellt wird, der für jeden LKW genaue Zeiten festlegt und der so auch eingehalten werden muss. Ohne eine feste Taktung können einfach die Potenziale zur Zeitersparnis nicht ausgenutzt werden. Ein realer Einsatz der hier entwickelten Algorithmen setzt also grundsätzlich voraus, dass diese Zeitplanung auch praxistauglich ist. LKW Fahrer müssten also vorab einen Slot in festzulegender Größe buchen, im späteren Verlauf wird ihnen nach abgeschlossener Zeitplanung eine feste Uhrzeit mitgeteilt, zu der sie sicher am Terminal erscheinen müssen. Durch hinzufügen von Pufferzeiten ließe sich hier sicherlich noch ein gewisser Spielraum erzeugen, aber Verschiebungen innerhalb des Zeitplans durch z.B. Stau oder welche Gründe auch immer durch die perfekt getaktete Reihenfolge nicht möglich \todo{Expertenmeinung dazu?!}. Es stellt sich also die Frage, ob Speditionen und LKW Fahrer ein solches Modell akzeptieren würden und vor allem ob sie dies auch einhalten können. Gibt es zu viele Ausfallzeiten durch verspätete LKW ist das ganze Modell für das Terminal auch hinfällig und nicht 
nutzbar. Eine weitere Unsicherheit in einem solch eng getakteten Zeitplan ist, wie genau die Schätzung der Abfertigungszeiten bzw. der Maschinenwechselzeiten wirklich möglich ist. Sicherlich ließen sich hier noch komplexere Berechnungen implementieren, aber eine genaue Schätzung könnte bei einer derart hohen Variablität von Waren durchaus schwierig werden. Setzt man hier zu hohe Pufferzeiten ein, sinkt das Optimierungspotenzial stark, zu gering geschätzte Zeiten bringen allerdings den ganzen Zeitplan durcheinander und haben große Auswirkungen auf alle nachfolgenden LKW. 





%- Hohe Schwankungen
%- Schwierige Vorhersage
%- Festlegung von Zeiten unumgänglich für eine Optimierung

%- Diskussion Praxistauglichkeit

%- Allgemein Feste Zeiträume nach Kategorie so gut schätzbar und in der Praxis sinnvoll?

%- Viele Aspkete abstrahiert und in den algos nicht berücksichtigt

%- Schwer mit vorgefertigten Verfahren in so einem konkreten Problem zu arbeiten

%- Fokus musste auf bestimtme Askepkte pro Algo gelegt werden



\todo{Richitge stelle?
Diskussion: Einbettung des Ganzen in eine reale Anwendung. 
 - Wie/Wo sloteingabe, welcher Slot auswählbar, was bei Verschiebungen?, Benachtigungen, Priokunden, ...}
